
TestForkEnvironment x
AgentTabular  x
TestAgentTabular  x
TestNstepTD x
NStepTDHelper x
låta PROB_RANDOM minska med episod x
snygga upp TestNstepTD  x
ScalerLogarithmic   x
klass NStepEpisodeRunner  x
TestNStepEpisodeRunner  x
påvisa större n ger snabbare konvergens  x

input vec length = antal states  x
AgentHelper  x
AgentNetwork x
TestAgentForkNeural  x
AgentForkNeural learn  x
NStepNeuralAgentTrainer - getMiniBatch  x
NStepNeuralAgentTrainer - setValuesInExperiencesInMiniBatch  x
NStepNeuralAgentTrainer - trainAgentFromExperiencesInMiniBatch x
TestNStepNeuralAgentTrainer x
Uppdatera nn varje steg  x
cvarför låser sig ibland  - behöver clip, learningRate viktigt x
ForkNeuralValueMemory konstruktor  x
studera några episoder så verkligen korrekt experience  x
ändra steps - studera epis x
value = 0 ibland  x
newDefault snyggare x
setValuesInExperiencesInMiniBatch - discount hänsyn  x
testa olika discount TestNStepNeuralAgentTrainer  x
test - innan terminal node skall discúnt ej påverka  x
givenStartingAtState7_whenTrainedWithMoreSteps_thenFasterLearning  x
clean up TestNStepNeuralAgentTrainer x
plantuml multi step  x
AgentAbstract  x
AgentForkTabular extends AgentAbstract  x
ta veck chooseRandomAction etc i AgentIterface  x
rita om planutml med AgentAbstract x
clean up
EnvironmentInterface - ta bort Set<Integer> stateSet()   - EnvironmentDiscreteStatesInterface blir för bökigt x
AgentInfo - lägg till getDiscountFactor   x
AgentInterface - ta bort getDiscountFactor  x
AgentAbstract - lägg till fält nofSteps  x
AgentInfo - kom åt nofSteps  x
Test för nofSteps  x
ny klass i AgentAbstract TemporalDifferenceTracker  x
AgentInfo - kom åt TemporalDifferenceTracker med olika filter tidsfönster  x
TestTemporalDifferenceTracker  x
plott ex trackTempDifferenceErrors x
trackTempDifferenceErrors i NStepNeuralAgentTrainer x
plotta ovan  x

generisc node x
whenActionIs0Or1InState14_thenState15AndTerminalAndRewardHell x
planutml med generisc node  x
NetworkMemoryInterface mer clean ISP  x

Annan miljö - tex Maze
MazeEnvironment x
TestMazeEnvironment  x
TestNStepTabularAgentTrainerMaze  x
function för start node  x
State = MazeVariables(x=1, y=5), value = 115.90638230139263  x
TestNStepNeuralAgentTrainerMaze  x
pusha  x
printMaze i TestHelper  x
printa G isf  x

skapa AgentTabularInterface med writeValue  x
writeValue skall ej behövas i agentAbstract  x
plantuml AgentTabularInterface  x

ny miljö slideInCharge x
TestChargeEnvironmentBTrapped x
TestChargeEnvironmentBRunning  x

AgentChargeGreedy  x
TestAgentChargeGreedy  x
ändra charge miljö till mer som P
- P charge started x
- gamla till trash x
- ChargeEnvironmentSettings x
- ChargeEnvironmentLambdas - skall ta emot ChargeEnvironmentSettings  x
- införliva ChargeEnvironmentSettings  x
- införliva ChargeEnvironmentLambdas x
- modda PositionTransitionRules x
- TestPositionTransitionRules  x
- modda SiteStateRules x
- TestSiteStateRules  x
- ChargeEnvironment: Pair<Integer, Integer> ->  Positions  x
- ChargeEnvironment: Pair<Double, Double> -> SoCLevels x
- TestCharge ..  x
- TestAgentChargeGreedy x
- fler tester TestAgentChargeGreedy - tex med obstacle x
- färstå "0,10,0.9,0.9, false,1,20" x
- clearner chooseBestAction x
- studera/clean up AgentChargeGreedy x
- clean up ChargeEnvironment , tex and or i lampda expr, x
- clean up trainers  x
- snyggare print node - soc många decimaler   x

StepReturn isNewStateFail andra miljör  x

AgentChargeGreedyRuleForChargeDecisionPoint  x
TestAgentChargeGreedyRuleForChargeDecisionPoint x
RunnerAgentChargeGreedyRuleForChargeDecisionPoint x
andel still que i ovan  x
TestAgentChargeGreedyRuleForChargeDecisionPoint x
pos A =7, pos B = 8 -> A stannar - bara que vis 20 straffades  x

AgentChargeNeural
- AgentChargeNeural init x
- AgentChargeNeuralSettings x
- NeuralValueMemoryCharge x
- InputVectorSetterChargeInterface x
- PositionMapper  x
- TestInputSetterSoCAtOccupiedZeroOther  x
- TestAgentNeuralChargeMockedData
- TestAgentNeuralChargeMockedData - givenRuleBasedValue_whenTrain_thenCorrect  x
- plot network error vs iteration  x
- ScalerLinear -> Normalizer  x
- NormalizerInterface  x
- NormalizerInterface -> NormalizerMinMax NormalizerMeanStd  x
- TestNormalizer  x
- apply Normalizer InputSetterSoCAtOccupiedZeroOther  x
- tester funkar x
- apply Normalizer out  x
- constructor i ValueMemoryNetworkAbstract  x
- NetSettings - nof Layers  x
- TestAgentNeuralChargeMockedData clean up - x
- Plotta egentiga felet  x
- RunnerChargeNeuralMocked - med olika inställmningar  x
- hot encoding positions+socs  x
- RunnerChargeNeuralMocked cleaner x
- MockedReplayBufferCreatorCharge i TestAgent..  x
- Andra TransferFunctionType  x
. AgentMazeNeuralSettings gör snyggare -utan getWithDiscountAndLearningRate x
- få TestAgentNeuralChargeMockedData stabil x
- momentum x

RunnnerAgentChargeNeuralBTrapped - Testa ovan när B är trapped x
- flytta discountFactor från AgentAbstract till AgentSettins  x
- olikma momentum x
- discount factor x
- reset memory x
- bugg still 20 x
- kör episoder x
- ChargeEnv energi minskar även om står still x
- RunnerAgentChargeNeuralTrainerBTrapped funkar x
- mindre max tid tränning  x
- simulering efter träning med större max tid  x
- plotta v20-v11 för olika soc x
- PlotterMultiplePanelsRelation  x
- ovan för 20-100 soc  x
- zoom in TD error plot  x

- prova resetta innan  x
- ny klass AgentEvaluator  x
- plotta G vs episode  x
- agent.setState(node) - missat annat ställe?  x
- word dokumnetera  x

Overall cleaning  x
Mer i hjälp klas RunnerBTrapped x

TestAgentChargeNeuralBothFree  x
RunnerNormalizerMeanStd - testa olika bad*x  x
test alpha =3,5  x
kunna spara nätverk  x
memory classes plantuml   x
ChargeInitStateVariantsEvaluator x
record Scenario - scenario ta in namn  x
RunnerChargeScenariosEvaluator x
BUGG RunnerChargeScenariosEvaluator ger olika results  x
bug fixad - missade sortering i PositionMapper x
- visa tid runner  x
sum reweards log trainer x
variant BatPosSplit_AatPos40_BothModerateSoC  x
TD error log trainer x
scenario 1000 steg - jämföra med rule based x
stabil körning RunnerAgentChargeNeuralTrainerBothFree x

HyperParameterOptimizer  c
mer data till ChargeAgentParameters  x
ScenarioEvaluator i RunnerAgentChargeGreedyRuleForChargeDecisionPoint  x
även mean td error i RunnerChargeHyperParameterOptimizer  x
tidsbegränsning per parameter uppsättning HyperParameterOptimizer  x
RunnerAgentChargeNeuralTrainerBothFreeParallelTasks x
call returnera tränad trainer  x
prova prio exp replay
- IntervalFinder x
- ExperiencePrioritizationSetter x
- TestExperiencePrioritizationSetter x
- ExperienceProbabilitySetter x
- TestExperienceProbabilitySetter x
- RunningSum x
- TestRunningSum x
- låta NStepNeuralAgentTrainer ta in godtycklig buffer  x
- ReplayBufferNStepPrioritized  x
- TestReplayBufferNStepPrioritized  x
- cleaner ReplayBufferNStepPrioritized  x
- BUGG End element in accumulated experiences differs from one, it is = 3.0 x
påvisa isTimeToUpdate inte ställer till det  x
- Testa intervallFinder när prob=0  x
- ExperienceWeightSetter  x
- TestExperienceWeightSetter  x
- test i TestReplayBufferNStepPrioritized för weight x
- method i memory som uppdaterar med olika vikter x
- testa ovan  x
- test NStepNeuralAgentTrainer med prio buffer - kanske på fork  x
- RunnerForkNStepNeuralAgentTrainer -> RunnerForkNStepNeuralAgentTrainer_UniformVersusPrioritizedBuffer  x
- BUGG accumulated experiences differs from one, it is = 2.3130667018961435  x
- Thus, β starts small (values of 0.4 to 0.6 are commonly used) and anneals towards one  x
- BUGG RunnerAgentChargeNeuralTrainerBothFreeParallelTasks verkar inte parallisera  x
- förstå hur agent används i trainer - hitta best action osv - kod snutt till volvo mail  x
- Runner på charge med prio buffer  x
- Multiple matching rules, nof =2  FINE x
- BUGG Runner på charge med prio buffer concurr mod exception  x
- ReplayBufferFactory  x

RunnerChargeHyperParameterOptimizerParallel  x
prova target network
- test kopiera nätverk x
- bevisa ursprungkligt ej påverkas när moddar kopierad  x
- kolla source target lika i copyWeights  x
- copyWeights i interface - sliper då skapa nya  x
- setValuesInExperiencesInMiniBatch använder backUpMinne
- target netwoek i trainer  x
- setValuesInExperiencesInMiniBatch - backup minne minne isf agent minne x

- skapa fork settings Rhell=Rheaven  x
- förstå varför tex fel i fork inte konvergerar mot noll    x
- studera olika discount rate och nof steps between i fork x
- om hell ger samma reward - noll td error  x
- stor nof steps between skall ge noll error i fork - ingen backup- varför ökar MeanTDErrormed steps between  x
- förstå NetSettings minOut(minOut).maxOut(maxOut)   - för clip  x

- test maze mer
- RunnerNeuralAgentTrainerMaze
- nofSteps i MazeVariables  maxStepsInEpisode i Maze  x
- policy remove item full buffer  x
- defaultIfNullObject  x
- RunnerAgentChargeNeuralTrainerBothFree - varför konvergerar så långsamt
- annan discouint rate RunnerAgentChargeNeuralTrainerBothFree x
- fixa RunnerAgentChargeNeuralTrainerBothFreeParallelTasks
- studera minibatch - går förstå varfr högt td error  x
- skicka in agentSettings till actionSelector  x

- oxå optimerra BUFFER_MAX_SIZE, DISCOUNT _RATE

- kommentera klasser

statistic (mean, std, etc) TD error i exp replay buffer

TestNStepNeuralAgentTrainerMaze fuckar ibland
TestForkNeuralValueMemory fuckar
AgentChargeNeural learn - ta bort addErrorsToHistory etc - onödig
ChargeScenariosEvaluator<S> göra generisk
mer till folder factories
Snabba upp genom prototyp

givenRuleBasedValue_whenTrain_thenCorrect fuckar
snygga upp plotters - tex inställningar i konstruktor
- MultiePanelScatterPlot
- MultiePanelScatterPlot
- grid plotter
- animering site

RunnerAgentChargeGreedyRuleForChargeDecisionPointObstacleSomeTimes med obstacle ibland

grafix för ovan

röriga konstruktors if AgentForkNeural

ide om två nn minnen - ett med större intervall med terminal fail fokus
för ovan testa ide grovt på mockad data på nät

- Ev sandbox example, 4 ingångar, ut litet om ngn av två första mindre än 0.5

ev target network
https://en.wikipedia.org/wiki/Encog
