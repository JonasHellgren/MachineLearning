env agent x
TestEnv x
TestAgent x
Trainer x
alternativ miljö setting upordown x
tester olika miljöer  x
städa upordown z
dokumentera i md upordown - tex renskriv bevis derGradLog x

two armed bandit working x
clean up x
accumSum med streams  x
gradLog med mer generell matte  x
TrainingTracker  x
runner som plottar phiTheta x
TrainingTrackerPlotter  x
plantuml bandit x

short corridor x
EnvironmentSC  x
TestEnvironmentSC  x
AgentSC x
korrekt stateObserved  x
TestAgentSC x
clean up  x
tracker.addActionProbabilities x
RunnerShortCorridor  x
shortCorridor.md  x

sink the ship  x
EnvironmentShip x
TestEnvironmentShip  x

TrainerAbstractSC, record TrainerSettings med nofEpisodes etc  x

short corridor baseline TrainerWithBaselineSC  x
shortCorridor.md om baseline  x
RunnerTrainerWithBaselineSC  x
short corridor one step actor critic  TrainerWithActorCriticSC  x
Experience.copyWithReturn()  x
ny klass TabularValueFunction, för tex TrainerActorCriticSC  x
uppdatera plantuml  x


AgentShip  x
TestAgentShip  x
calcGradLogVector  createGradLogAllStates x
givenState0_whenGradLogForActionMeanS1_thenCorrect x
TrainerShip x
RunnerTrainerActorCriticShip  x
korrekt gradlog x
sinkShip.md  x
undivka div noll gradMean x
clip gragLog vektor x
sqr2 etc till MyFunctions x
tracker.addActionProbabilities  -> addMeasures etc  x
getExperiences: while(!sr.isTerminal() && si < parameters.nofStepsMax()) i andra miljör  x
TrainerAbstract  x
NormDistributionSampler x
ExperienceContAction Experience, båda!! går göra snyggare?  x
kommentera klasser  x
clean up


förstå https://www.janisklaise.com/post/rl-policy-gradients/
koda ovan
EnvironmentPole x
kolla av och snygga upp EnvironmentPole  x
TestEnvironmentPole  x
AgentPole x
TestAgentPole  x
TrainerVanillaPole  x
TestTrainerVanillaPole x
RunnerTrainersPole x
TrainerBaselinePole x
BaselinePole i runner  x
NStepReturnInfoPole x
TestNStepReturnInfoPole  x
TestTrainerActorCriticPole x
pseudocode n-step AC  x
få till AC utan minne stort n X
enkla dl4j exempel
få till nätverk som tar in pole states
testträna ovan
införliva i n-step AC x
FitSum2  x
Normalizer x
FitSum2 normalzed x
clener x
NeuralValueFunctionPole  x
normalizer i ovan  x
TestNeuralValueFunctionPole x
TestNormalizer  x
kunnna anropa train flera ggr utan create etc  x
normarizer out NeuralSum x
oven men i NeuralPole  x
slumpstate test i TestNeuralValueFunctionPole x
NeuralValueFunctionPole 2 hidden x
valueFunction med avg of n-step
RunnerTrainersPole
TrainerActorCriticPole med Neural mem x
correct AC x

uppdatera pseudocode AC  x
pseudocode andra  x
beta -> learningRateCritic, learningRateActor -> learningRateActor  x
ParametersPole clean with builder consttuccotr  x
trainer AC konvergerar ej bra x

customloss test x
Dl4j pg loss x
CustomPolicyGradientLoss x
RunnerCustomPolicyGradientLoss x
AgentBanditNeural  x

MultiLayerNetworkCreator x
numerisk gradient dl4j x
Using ND4J with AVX512 will improve performance x
MultiLayerNetworkCreator på fler ställen  x
md fil om entropy etc x
möjligt få StatePole och ExperiencePole generiska, dvs implementera Interface x
ny record Action, antingen int eller double  x
Action med optional x
AgentThetaActorInterface i plantuml etc  x
skapa AgentThetaActorInterface etc x
skapa AgentAbstract, innehåller state  x
namnbyte  Interface -> I Abstract -> A x
skapa generisk Experience  x
skapa generisk StepReturn  x
NeuralMemoryPole memory;  //todo flytta till agent
skapa generisk ThetaActorTrainer
ovan på bandit
fixa gitpush
skapa generisk NeuralActorTrainer
skapa generisk ThetaCriticNeuralActorTrainer
skapa generisk NeuralCriticNeuralActorTrainer


stepHorizon(1) buggar
short corridor med neural policy agent
cart pole ACNeuralPolicy

cart pole PPOPolicy

dokumentera public methoder etc

animering i pole runner med färdig AC policy

läsa mcts kod
strategi sätta ihop mcts med pg problems
alphazero på cartpole



bubblare:
gradLog with finite diff