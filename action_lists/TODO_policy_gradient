env agent x
TestEnv x
TestAgent x
Trainer x
alternativ miljö setting upordown x
tester olika miljöer  x
städa upordown z
dokumentera i md upordown - tex renskriv bevis derGradLog x

two armed bandit working x
clean up x
accumSum med streams  x
gradLog med mer generell matte  x
TrainingTracker  x
runner som plottar phiTheta x
TrainingTrackerPlotter  x
plantuml bandit x

short corridor x
EnvironmentSC  x
TestEnvironmentSC  x
AgentSC x
korrekt stateObserved  x
TestAgentSC x
clean up  x
tracker.addActionProbabilities x
RunnerShortCorridor  x
shortCorridor.md  x

sink the ship  x
EnvironmentShip x
TestEnvironmentShip  x

TrainerAbstractSC, record TrainerSettings med nofEpisodes etc  x

short corridor baseline TrainerWithBaselineSC  x
shortCorridor.md om baseline  x
RunnerTrainerWithBaselineSC  x
short corridor one step actor critic  TrainerWithActorCriticSC  x
Experience.copyWithReturn()  x
ny klass TabularValueFunction, för tex TrainerActorCriticSC  x
uppdatera plantuml  x


AgentShip  x
TestAgentShip  x
calcGradLogVector  createGradLogAllStates x
givenState0_whenGradLogForActionMeanS1_thenCorrect x
TrainerShip x
RunnerTrainerActorCriticShip  x
korrekt gradlog x
sinkShip.md  x
undivka div noll gradMean x
clip gragLog vektor x
sqr2 etc till MyFunctions x
tracker.addActionProbabilities  -> addMeasures etc  x
getExperiences: while(!sr.isTerminal() && si < parameters.nofStepsMax()) i andra miljör  x
TrainerAbstract  x
NormDistributionSampler x
ExperienceContAction Experience, båda!! går göra snyggare?  x
kommentera klasser  x
clean up


förstå https://www.janisklaise.com/post/rl-policy-gradients/
koda ovan
EnvironmentPole x
kolla av och snygga upp EnvironmentPole  x
TestEnvironmentPole  x
AgentPole x
TestAgentPole  x
TrainerVanillaPole  x
TestTrainerVanillaPole x
RunnerTrainersPole x
TrainerBaselinePole x
BaselinePole i runner  x
NStepReturnInfoPole x
TestNStepReturnInfoPole  x
TestTrainerActorCriticPole x
pseudocode n-step AC  x
få till AC utan minne stort n X
enkla dl4j exempel
få till nätverk som tar in pole states
testträna ovan
införliva i n-step AC x
FitSum2  x
Normalizer x
FitSum2 normalzed x
clener x
NeuralValueFunctionPole  x
normalizer i ovan  x
TestNeuralValueFunctionPole x
TestNormalizer  x
kunnna anropa train flera ggr utan create etc  x
normarizer out NeuralSum x
oven men i NeuralPole  x
slumpstate test i TestNeuralValueFunctionPole x
NeuralValueFunctionPole 2 hidden x
valueFunction med avg of n-step
RunnerTrainersPole
TrainerActorCriticPole med Neural mem x
correct AC x

uppdatera pseudocode AC  x
pseudocode andra  x
beta -> learningRateCritic, learningRateNonNeuralActor -> learningRateNonNeuralActor  x
ParametersPole clean with builder consttuccotr  x
trainer AC konvergerar ej bra x

customloss test x
Dl4j pg loss x
CustomPolicyGradientLoss x
RunnerCustomPolicyGradientLoss x
AgentBanditNeural  x

MultiLayerNetworkCreator x
numerisk gradient dl4j x
Using ND4J with AVX512 will improve performance x
MultiLayerNetworkCreator på fler ställen  x
md fil om entropy etc x
möjligt få StatePole och ExperiencePole generiska, dvs implementera Interface x
ny record Action, antingen int eller double  x
Action med optional x
AgentThetaActorInterface i plantuml etc  x
skapa AgentThetaActorInterface etc x
skapa AgentAbstract, innehåller state  x
namnbyte  Interface -> I Abstract -> A x
skapa generisk Experience  x
skapa generisk StepReturn  x
Generic klasser på bandit  x
skapa generisk ParamActorTrainer x
ovan på bandit x
skapa generisk NeuralActorTrainer x
fixa gitpush x
generics på ship  x
valueFunction i agent x
ParamActorTabularCriticTrainer x
sc generisk  x
valueFunction i agent sc  x
clean up sc  x
generisk trainer sc x

pole generisk  x
AgentPole -> AgentParamActorPole, AgentParamActorNeuralCriticPole  x
skapa AgentParamActorI x
TrainerVanillaPole använder AgentParamActorPole x
TrainerParamActorNeuralCriticPole använder AgentParamActorPoleNeuralCritic x
TrainerVanillaPole använder generisk epsiode trainer  x
AgentSC -> AgentParamActorSC, AgentParamActorTabCriticSC  x
AgentParamActorTabCriticI ersätt getCritic  x

skapa MultistepNeuralAgentHelper x
använd ovan i TrainerParamActorNeuralCriticPole x
plantnum AgentI etc x
MultistepParamActorUpdater x
generell pg clean up  x
TestReturnCalculator - visade sig bugga x
rensa, tex ExperienceOld  x

AgentI genomtänk  x
använda final och abstract på trainer klasser  x
createExperienceListWithReturns i TrainerAbstract ta bort?  x
double I = 1; bara i ParamActorTabCriticEpisodeTrainer  - fimpat x
NeuralActor: fitActor(INDArray in, INDArray out) -> fitActor(List<List>> in, List<> INDArray out);  x
whenCreated_thenSameNumAsAnalyticGrad buggar   x
CustomPolicyGradientLoss.newNumDefault() buggar  x
short corridor med neural actor-critic agent - AgentNeuralActorNeuralCriticSC  x
NeuralActorMemorySC  NeuralCriticMemorySC x
TestNeuralActorMemorySC x
EntropyCalculator x
TestEntropyCalculator x
EntropyCalculator i CustomPGLoss  x
bugg - problem när Gt -1  - Loss=−log(P(a∣s))⋅G  +β⋅H(P)  - löst med eps i  EntropyCalculator x
TestNeuralCriticMemorySC  x
realisera AgentNeuralActorNeuralCriticSC  x
TestAgentNeuralActorNeuralCriticSC x
AgentNeuralActorNeuralCriticSC buggar x
prova one hot neural AC actor x
hittat och fixt bugg i AgentNeuralActorNeuralCriticSC - ej obs state x
one hot även för critic x
minor clean up x
getObservedPos createOneHot etc i StateSC  x
skippa norm one hot actor och critic SC  x
bara printa state values efter träning x
kunna påverka hur stokastisk med entropy loss i CustomPolicyGradientLoss x
Experience med isTerminal x
skapa generisk NeuralCriticNeuralActorTrainer x
koda AgentNeuralActorNeuralCriticPole
NeuralActorMemoryPole x
TestNeuralActorMemoryPole x
NeuralActorMemoryPole normalized out  x
TestTrainerNeuralActorNeuralCriticPole konvergarar inte

TrainerMultiStepNeuralActorNeuralCriticPole x
RunCustomLoss2Out x
CustomLoss med num gradient  x

whenCreatedManyPoints_thenCorrectGrad x
TestCustomPolicyGradientLoss med correct shape på out  x
NeuralActorEpisodeTrainer - använd trainFromEpisodeNew  x

NeuralActorMemorySC använda CustomPolicyGradientLossNew x
Dl4JBatchNetFitter x
Double relativeNofFitsPerBatch,        int sizeBatch  -> NetSettings  x

Dl4JBatchNetFitter på AgentBanditNeuralActor  x
Dl4JBatchNetFitter på NeuralMemorySum  x
Dl4JBatchNetFitter - test using all points, not just sub set in batch
Dl4JBatchNetFitter på NeuralCriticMemorySC  x
TestTrainerParamActorNeuralCriticPole fuckar  x
INDarray exampel  x
batch ex x
Dl4JBatchNetFitter korrekt?  x
Dl4JBatchNetFitter på NeuralCriticMemoryPole  x
Dl4JBatchNetFitter på NeuralActorMemoryPole x
CustomPolicyGradientLossNew - ersätta scoreOnePoint  x
absNoFit parameter - påv fitter  x
Dl4JUtil - convertListOfLists snyggare och snabbare - prealloc INDArray  x
Dl4JUtil putRow  x
Dl4JBatchNetFitter på NeuralActorMemoryPole - stabil  x

fimpa convertListOfLists nofinp  x
behövs Dl4JNetFitter ?  x
fitActor med nofFits  x
TestTrainerNeuralActorNeuralCriticPole  x
CustomPolicyGradientLossNew -> CustomPolicyGradientLoss - ta bort äldre ver x
samma med NumericalGradCalculatorNew  x
NeuralActor<V>   List<List...>  x
få CustomPolicyGradientLoss klara batch x
TrainerMultiStepNeuralActorNeuralCriticPole cleaner  x
StatePole.nofActions() StatePole.nofStates()  x
fitActorOld veck  x
Neural ac pole i runner  x
uppdatera md fil one step ac  x

RunnerShortCorridor fuckar för neural - lägre learning ratefixar  x
ta bort TrainerI  x
MultiStepResults som record och egen klass x
package struktur  - domain helpers runners  x
läsa testkod  x
TrainerParameters learningRate påv ej - ev ta bort  x
common_generic + common_value_classes -> value_classes  x
MultistepNeuralCriticUpdater är svår förstå  x
VariablesPole calcNew - följa StatePole innehar VariablesPole, PoleRelations, PoleParameters x
döpa om TrainerVanillaPole till mfl TrainerParameterActorPole  x
behövs alla trainers? x
flytta några helpers till common, tex BucketLimitsHandler
ResultManySteps som record  x
MultiStepResultsGenerator NeuralCriticUpdater  NeuralActorUpdater x
fler tester helpers x
TestMultiStepResultsGenerator x
MultistepParamActorUpdater  ersätt fält med agent  x
ref bilder i markdown  x
Pseudocode PPO x
påbörja CustomPPOLoss  x
FiniteDifferenceCalculator  x
TestCustomPPOLoss  x
-
psudo code ppo multi step  x
psudo code multi step  x
uppdatera multi step return def  - terminal isf fail etc  x

ny environment - MultiCoinDenominationsBandit   x
TestMultiCoinDenominationsBandit  x
MultiCoinBanditAgentPPO  x
NeuralActorPPOLossTrainer x
TrainerMultiCoinBanditPPO x
RunnerMultiCoinBandit x
mer printing TestTrainerMultiCoinBanditAgentPPO - förstå  x
mer clean up
- cleaner CrossEntropyLoss  x
- namn som anger CrossEntropyLoss  x
-computeGradientAndScore x
- trainFromEpisode på flera ställen göra generisk
- NeuralActorCrossEntropyLossTrainer + NeuralActorPPOLossTrainer -> NeuralActorTrainer  x
- fitActor INDArray direkt - skippar tills vidare verkar inte sakta ner så mkt x

TrainerMultiStepNeuralActorNeuralCriticPole -> TrainerMultiStepActorCriticPole  x
NeuralActorUpdater generisk genom interface  x
TestTrainerMultiStepActorCriticPolePPOLoss x
cart pole PPOPolicy x
entropy på PPO loss  x
entropy på PPO loss beta = 0.1;  //todo in constructior  x

förklara Weighted cross entropy loss  x
döm om klasser med CntropyLoss  -> CEM  x

Training Recoders x
Probability   Lik nuvarande x
implemetera ovan  MultiCoinBandit TwoBandit SC x
RecorderStateValues  x
TrainerAbstractShip med RecorderStateValues  x
episodeMeasuresMap -> List<Map<Integer, List<Double>>   episodeProbabilitiesList   // every list element is a map <state,probabilities >  x

Training.   sumRewards nSteps criticLoss actorLoss etc
RecorderTrainingMeasures x
TestRecorderTrainingMeasures x
införliva RecorderTrainingMeasures x
RecorderEvaluationProgress  x
TestRecorderEvaluationProgress  x
införliva fler Rl measures
införliva RecorderEvaluationProgress i sc x
införliva RecorderEvaluationProgress i cart pole x
NeuralActorMemoryPolePPOLoss  NeuralActorMemoryPoleCrossEntropyLoss  lika  x

NeuralActorMemorySC -> NeuralActorMemorySCLossCEM  NeuralActorMemorySCLossPPO  x
AgentActorCriticSCLossPPO  x
NeuralActorI - ny method actorOut(state)  x
VariablesSC add observedPos  x
fixa TestNeuralCriticMemorySC  x
TrainerNeuralActorNeuralCriticSC flexibel agent x
fimpa getActionProbabilities i interface  x
RunnerShortCorridor lägg till PPO  x

RecorderActionProbabilities add plot()  x

maze with ppo
- TestRobotMover x
- EnvironmentMaze x
- TestEnvironmentMaze x
- clean up  x
- NeuralActorMemoryMaze x
- NeuralActorMemoryMazeLossPPO x
- TestNeuralActorMemoryMazeLossPPO x x
- TestNeuralCriticMemoryMazeLossPPO
-  double probAction=0;  //todo  x
- plotta critic maze  x
- plotta policy maze x
- TrainerMazeAgentSingleStepPPO  x
- bara lära sig om term state  x
- MazeAgentPPO
 -  settings konstuktor för enklare miljö  x
 - testa ovan x
- TrainerMazeAgentPPO på enkel settings x
- BUGG - TrainerMazeAgentMultiStepPPO - för stor critic value  x
- newOneRowMoveAsIntended  x
- new4x3MoveAsIntended x
- kolla av korrekt value  x

plotta entropy pole  x
pole med random start state och eval recorder  x
AgentParamActorSCHelper - gör för mkt  x

safe cannon with ppo

RunnerActorCriticShipPPO
AgentShipPPO x
    NeuralActorMemoryShip actor;  x
    NeuralCriticMemoryShip critic; x
TestAgentShipPPO



simplified battery trading with safe RL ppo

StateI asArray
MultiStepResultsGenerator - fimpa : nofExperiences - n + 1;

bubblare/ned prioriterat:
dokumentera public methoder etc
animering i pole runner med färdig AC policy
ArrayRealVector RealVector rörs ihop
hur skall AgentNeuralActorI kunna klara cont action
public class Test till runner
ta bort AgentParamActorNeuralCriticPole
TrainerBaselinePole använda generisk tränare
gradLog with finite diff
skapa generisk ParamActorNeuralMultiStepCriticTrainer
TrainerParamActorNeuralCriticPole använder ParamActorNeuralMultiStepCriticTrainer

läsa mcts kod
strategi sätta ihop mcts med pg problems
alphazero på cartpole