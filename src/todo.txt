


undvika calcOutX calcOutY, sätt flip som klass parameter  x
marginal i Frame  X
skapa separat package: Java based open GYM  x

Environment som abstract isf interface x
getParameters();  //polymorphism: can return any sub class of EnvironmentParametersAbstract  stämmer?? behövs getParameters? onödig  X
Agent interface döps om till Learnable x
Bas klass för agent tabular x
Bas klass för agent nn  x
skapa method för "INDArray inputNetwork = state.getStateVariablesAsNetworkInput(envParams)"  x
Warning: Initializing ND4J with Generic x86 binary on a CPU with AVX/AVX2 support
testa med regel baserad action för MountainCar, tex 2 om pos>tbd och 0 om pos<tbd  X
startPosition verkar fel x
printa pos and speed i panel  x
förstå varför grafik ej visas ibland x
MountainCar agent
---------------
inte behöva träna nn varje steg  x
ibland vissa policy för episode med start i tbd  x
träna olika värden olika states x
learnAtDifferentInputsZeroGamma  FUCKAR  !!
skap PanelMountainCarPlot  x
plotta policy x
antal steg isf antal episoder avgör networkTarget uppdatering etc x
prova SixRooms  x
learnAtSameInputStandardRewardNonZeroGamma verkar krångla   x
kolla av nof steps, nollställa vid rätt tillfälla. totalNofSteps i state, nofFits i AgentNetwork, nofSteps i "specific" State.
snygga upp kod, tex några metoder från test klass in i andra klasser,   x
stepReturn.reward=-1 för learnAtSameInputStandardRewardNonZeroGamma
undvika kalla createContinuousVariable flera ggr x
skapa seperata test för funk approx  resp inlärning  x
alternativ reward
lära sig Q för regelabserad, väl lika mellan actions x
nofFits:2376, totalNofSteps:1227936 stämmer inte  x
nyanlända experience, prioritet?  x
normalisiering   x
ändra range  x
annan nn arkitekur, testat LEAKRELU, funkar sämre  x

testa SixRooms  x

code clean up:
- agent.state.totalNofSteps % agent.NOF_STEPS_BETWEEN_FITS == 0 som function  x
- flytta state.totalNofSteps++;   VET EJ HUR
- render function  x
- render function i abstract class
minska q learning rate   x
animera under träning   x
färga circle sfa action i animering  x
pusha till repo  x

snyggare textning i anim fönster  x
mer info i anim fönster, tex max Qsa   x
0 <=> uniform distribution  = 0.9
testPolicy printa mer än succes ratio, tex bellman error, antal steg
plotta policy, prickar med olika färg

code clean up
- bantad ScaleLinear: LinearScaler i ModelsCommon
- förklara RB_EPS etc

eps decay  x
momentum x  verkar bra med stort, typ 0.99
mer clipping  olika BE_ERROR_MAX
minska ALPHA
studera minibatch
minibatch ej exp rep
annan w init
min avg max på weights
exploderar w utan gradientNormalization?


annan environment, tex chart pole  x
cart pole agent  x
pole reward från fail state  x
göra test policy mer flexibel  nja, verker ej gå   x
långsammare slutanimering  x
spara bästa policy hittils x
hyper par från artikel
annan skalning på input    x
prova mindre massa cart   verkar ej påverka nämnvärt, mest försvåra x
enkom cart ändra färg sfa action  x
mer exp replay x

clean up
- state isf agent till render  x
- animatePolicy till Environment  x
- newState och stepReturn, flytta från step  skippar  x
- döp om badstate till fail state Environment  x
- load and save methoder i base NnAgentClass  x
- diverse tex calcRandomFromIntervall til class MathUtils  x


spara bästa policy hittils även för mountain car  x
fail state fcn för mountain car  x

printa [min,avg,max] nofSteps  x
plantuml  x
setNetworkInput(state, envParams)     envParams verkar inte behövas här och på andra ställen
beskrivande .md fil
pong environment

när tid:
test fcn approx på six rooms
binary string variable i State   https://stackoverflow.com/questions/7602665/store-an-array-in-hashmap/7602742
prova relu som activ function för six rooms
GPU vs CPU för riktigt stor nn/batch
normalize regression examples
udemu ai4 bandit example
Test saving fail states-actions