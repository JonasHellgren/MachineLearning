# Continuous action selection

Policy gradient theorem gives policy parameter updates

![update.png](..%2Fpics%2Fupdate.png)

where nabla J is

![ploicygradient_with_adv.png](..%2Fpics%2Fploicygradient_with_adv.png)


actions are sampled from pdf

![pdf.png](..%2Fpics%2Fpdf.png)


![gaussian.png](..%2Fpics%2Fgaussian.png)


Continuous action RL is about updating mean and std


![advantage.png](..%2Fpics%2Fadvantage.png)
  

    return=sum of rewards


![gradLog.png](..%2Fpics%2FgradLog.png)